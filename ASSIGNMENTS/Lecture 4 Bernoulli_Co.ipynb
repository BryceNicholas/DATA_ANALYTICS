{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee2b75fa-7a45-42ff-abe6-91ba6c3490ca",
   "metadata": {},
   "source": [
    "1. Read the Bernoulli Mixture Model Derivation.\n",
    "2. Read about Stochastic Expectation-Maximization (EM) Algorithm: https://www.sciencedirect.com/science/article/pii/S0167947320302504.\n",
    "3. From the given code, modify the EM algorithm to become a Stochastic EM Algorithm.\n",
    "4. Use the data from the paper: https://www.sciencedirect.com/science/article/abs/pii/S0031320322001753\n",
    "5. Perform categorical clustering using the Bernoulli Mixture Model with Stochastic EM Algorithm.\n",
    "6. Compare its performance with K-Modes Algorithm using Folkes-Mallows Index, Adjusted Rand Index, and Normalized Mutual Information Score.\n",
    "7. Compare and contrast the performances, and explain what is happening (i.e. why is FMI always higher than ARI and NMI? Why is ARI and NMI low compared to FMI? etc.)\n",
    "8. Write the report in Latex, push to your github with the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df91c501-7b08-4070-84d9-541463ad4b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 111, 'name': 'Zoo', 'repository_url': 'https://archive.ics.uci.edu/dataset/111/zoo', 'data_url': 'https://archive.ics.uci.edu/static/public/111/data.csv', 'abstract': 'Artificial, 7 classes of animals', 'area': 'Biology', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 101, 'num_features': 16, 'feature_types': ['Categorical', 'Integer'], 'demographics': [], 'target_col': ['type'], 'index_col': ['animal_name'], 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1990, 'last_updated': 'Fri Sep 15 2023', 'dataset_doi': '10.24432/C5R59V', 'creators': ['Richard Forsyth'], 'intro_paper': None, 'additional_info': {'summary': 'A simple database containing 17 Boolean-valued attributes.  The \"type\" attribute appears to be the class attribute.  Here is a breakdown of which animals are in which type: (I find it unusual that there are 2 instances of \"frog\" and one of \"girl\"!)', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '   1. animal name:      Unique for each instance\\r\\n   2. hair:\\t\\tBoolean\\r\\n   3. feathers:\\t\\tBoolean\\r\\n   4. eggs:\\t\\tBoolean\\r\\n   5. milk:\\t\\tBoolean\\r\\n   6. airborne:\\t\\tBoolean\\r\\n   7. aquatic:\\t\\tBoolean\\r\\n   8. predator:\\t\\tBoolean\\r\\n   9. toothed:\\t\\tBoolean\\r\\n  10. backbone:\\t\\tBoolean\\r\\n  11. breathes:\\t\\tBoolean\\r\\n  12. venomous:\\t\\tBoolean\\r\\n  13. fins:\\t\\tBoolean\\r\\n  14. legs:\\t\\tNumeric (set of values: {0,2,4,5,6,8})\\r\\n  15. tail:\\t\\tBoolean\\r\\n  16. domestic:\\t\\tBoolean\\r\\n  17. catsize:\\t\\tBoolean\\r\\n  18. type:\\t\\tNumeric (integer values in range [1,7])', 'citation': None}}\n",
      "           name     role         type demographic description units  \\\n",
      "0   animal_name       ID  Categorical        None        None  None   \n",
      "1          hair  Feature       Binary        None        None  None   \n",
      "2      feathers  Feature       Binary        None        None  None   \n",
      "3          eggs  Feature       Binary        None        None  None   \n",
      "4          milk  Feature       Binary        None        None  None   \n",
      "5      airborne  Feature       Binary        None        None  None   \n",
      "6       aquatic  Feature       Binary        None        None  None   \n",
      "7      predator  Feature       Binary        None        None  None   \n",
      "8       toothed  Feature       Binary        None        None  None   \n",
      "9      backbone  Feature       Binary        None        None  None   \n",
      "10     breathes  Feature       Binary        None        None  None   \n",
      "11     venomous  Feature       Binary        None        None  None   \n",
      "12         fins  Feature       Binary        None        None  None   \n",
      "13         legs  Feature  Categorical        None        None  None   \n",
      "14         tail  Feature       Binary        None        None  None   \n",
      "15     domestic  Feature       Binary        None        None  None   \n",
      "16      catsize  Feature       Binary        None        None  None   \n",
      "17         type   Target  Categorical        None        None  None   \n",
      "\n",
      "   missing_values  \n",
      "0              no  \n",
      "1              no  \n",
      "2              no  \n",
      "3              no  \n",
      "4              no  \n",
      "5              no  \n",
      "6              no  \n",
      "7              no  \n",
      "8              no  \n",
      "9              no  \n",
      "10             no  \n",
      "11             no  \n",
      "12             no  \n",
      "13             no  \n",
      "14             no  \n",
      "15             no  \n",
      "16             no  \n",
      "17             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "zoo = fetch_ucirepo(id=111) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = zoo.data.features \n",
    "y = zoo.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(zoo.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(zoo.variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbca1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "import numpy as np\n",
    "\n",
    "class BernoulliMixturewSEM:\n",
    "    \n",
    "    def __init__(self, n_components, max_iter, batch_size=10, tol=1e-3):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.tol = tol\n",
    "    \n",
    "    def fit(self, x):\n",
    "        self.x = x\n",
    "        self.init_params()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.old_logL = self.get_log_likelihood(log_bernoullis)\n",
    "        for step in range(self.max_iter):\n",
    "            if step > 0:\n",
    "                self.old_logL = self.logL\n",
    "            for batch in self.iterate_batches():\n",
    "                self.gamma = self.get_responsibilities(self.get_log_bernoullis(batch))\n",
    "                self.remember_params()\n",
    "                self.get_Neff()\n",
    "                self.get_mu(batch)\n",
    "                self.get_pi()\n",
    "            log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "            self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "            if np.isnan(self.logL):\n",
    "                self.reset_params()\n",
    "                print(self.logL)\n",
    "                break\n",
    "\n",
    "    def iterate_batches(self):\n",
    "        n_samples = len(self.x)\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        for start_idx in range(0, n_samples, self.batch_size):\n",
    "            end_idx = min(start_idx + self.batch_size, n_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            yield self.x.iloc[batch_indices]\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.mu = self.old_mu.copy()\n",
    "        self.pi = self.old_pi.copy()\n",
    "        self.gamma = self.old_gamma.copy()\n",
    "        self.get_Neff()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "        \n",
    "    def remember_params(self):\n",
    "        self.old_mu = self.mu.copy()\n",
    "        self.old_pi = self.pi.copy()\n",
    "        self.old_gamma = self.gamma.copy()\n",
    "    \n",
    "    def init_params(self):\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.n_features = self.x.shape[1]\n",
    "        self.pi = 1/self.n_components * np.ones(self.n_components)\n",
    "        self.mu = np.random.RandomState(seed=0).uniform(low=0.25, high=0.75, size=(self.n_components, self.n_features))\n",
    "        self.normalize_mu()\n",
    "        self.old_mu = None\n",
    "        self.old_pi = None\n",
    "        self.old_gamma = None\n",
    "    \n",
    "    def normalize_mu(self):\n",
    "        sum_over_features = np.sum(self.mu, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            self.mu[k,:] /= sum_over_features[k]\n",
    "            \n",
    "    def get_responsibilities(self, log_bernoullis):\n",
    "        gamma = np.zeros(shape=(log_bernoullis.shape[0], self.n_components))\n",
    "        Z =  logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            gamma[:, k] = np.exp(np.log(self.pi[k]) + log_bernoullis[:,k] - Z)\n",
    "        return gamma\n",
    "        \n",
    "    def get_log_bernoullis(self, x):\n",
    "        log_bernoullis = self.get_save_single(x, self.mu)\n",
    "        log_bernoullis += self.get_save_single(1-x, 1-self.mu)\n",
    "        return log_bernoullis\n",
    "    \n",
    "    def get_save_single(self, x, mu):\n",
    "        epsilon = 1e-15\n",
    "        mu_place = np.clip(mu, epsilon, 1 - epsilon)\n",
    "        return np.tensordot(x, np.log(mu_place), (1,1))\n",
    "\n",
    "    def get_Neff(self):\n",
    "        self.Neff = np.sum(self.gamma, axis=0)\n",
    "    \n",
    "    def get_mu(self, batch):\n",
    "        self.mu = np.einsum('ik,id -> kd', self.gamma, batch) / self.Neff[:, None]\n",
    "        \n",
    "    def get_pi(self):\n",
    "        self.pi = self.Neff / self.n_samples\n",
    "    \n",
    "    def predict(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        gamma = self.get_responsibilities(log_bernoullis)\n",
    "        return np.argmax(gamma, axis=1)\n",
    "        \n",
    "    def get_sample_log_likelihood(self, log_bernoullis):\n",
    "        return logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "    \n",
    "    def get_log_likelihood(self, log_bernoullis):\n",
    "        return np.mean(self.get_sample_log_likelihood(log_bernoullis))\n",
    "        \n",
    "    def score(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_log_likelihood(log_bernoullis)\n",
    "    \n",
    "    def score_samples(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_sample_log_likelihood(log_bernoullis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d727b30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Dataset             Algorithm       FMI       ARI       NMI\n",
      "0     zoo  BernoulliMixturewSEM  0.709483  0.000000  0.000000\n",
      "1     zoo                KModes  0.672672  0.334538  0.255743\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import fowlkes_mallows_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from kmodes.kmodes import KModes\n",
    "\n",
    "class CustomClusteringEvaluator:\n",
    "    def __init__(self):\n",
    "        self.encoded_datasets = {}\n",
    "    \n",
    "    def encode_categorical(self, df):\n",
    "        encoder = LabelEncoder()\n",
    "        encoded_df = df.copy()\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype == 'object':\n",
    "                encoded_df[column] = encoder.fit_transform(df[column])\n",
    "        return encoded_df\n",
    "    \n",
    "    def evaluate_clustering(self, dataset_name, algorithm, **kwargs):\n",
    "        dataset = self.encoded_datasets[dataset_name]\n",
    "        X = dataset.iloc[:, :-1]\n",
    "        true_labels = dataset.iloc[:, -1]\n",
    "        \n",
    "        if algorithm == 'BernoulliMixturewSEM':\n",
    "            model = BernoulliMixturewSEM(**kwargs)\n",
    "            model.fit(X)\n",
    "            labels = model.predict(X)\n",
    "        elif algorithm == 'KModes':\n",
    "            km = KModes(**kwargs)\n",
    "            labels = km.fit_predict(X)\n",
    "        else:\n",
    "            raise ValueError(\"Algorithm not supported.\")\n",
    "        \n",
    "        fmi, ari, nmi = None, None, None\n",
    "        \n",
    "        if true_labels is not None:\n",
    "            fmi = fowlkes_mallows_score(true_labels, labels)\n",
    "            ari = adjusted_rand_score(true_labels, labels)\n",
    "            nmi = normalized_mutual_info_score(true_labels, labels)\n",
    "        \n",
    "        return fmi, ari, nmi\n",
    "    \n",
    "    def run_evaluation(self, df):\n",
    "        results = {}\n",
    "        algorithms = ['BernoulliMixturewSEM', 'KModes']\n",
    "        dataset_name = 'zoo'\n",
    "        self.encoded_datasets[dataset_name] = self.encode_categorical(df)\n",
    "        results[dataset_name] = {}\n",
    "        for algorithm in algorithms:\n",
    "            if algorithm == 'BernoulliMixturewSEM':\n",
    "                kwargs = {'n_components': 2, 'max_iter': 100}\n",
    "            elif algorithm == 'KModes':\n",
    "                kwargs = {'n_clusters': 2, 'max_iter': 100}\n",
    "            fmi, ari, nmi = self.evaluate_clustering(dataset_name, algorithm, **kwargs)\n",
    "            results[dataset_name][algorithm] = {'FMI': fmi, 'ARI': ari, 'NMI': nmi}\n",
    "        \n",
    "        data = []\n",
    "        for dataset_name, algorithms in results.items():\n",
    "            if dataset_name == 'zoo':\n",
    "                for algorithm, metrics in algorithms.items():\n",
    "                    data.append([dataset_name, algorithm, metrics['FMI'], metrics['ARI'], metrics['NMI']])\n",
    "        \n",
    "        results_df = pd.DataFrame(data, columns=['Dataset', 'Algorithm', 'FMI', 'ARI', 'NMI'])\n",
    "        return results_df\n",
    "\n",
    "# Instantiate the evaluator and run the evaluation\n",
    "evaluator = CustomClusteringEvaluator()\n",
    "results_df = evaluator.run_evaluation(zoo.data.features)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5eccee",
   "metadata": {},
   "source": [
    "When comparing the performance of the Bernoulli Mixture Model with Stochastic EM Algorithm and the K-Modes Algorithm using FMI, ARI, and NMI, it's clear that FMI consistently gives higher scores than ARI and NMI. This difference is because FMI focuses on cluster similarities through pairwise agreements, leading to higher values when clusters are similar. In contrast, ARI considers both agreements and disagreements, penalizing random assignments; while NMI is sensitive to differences in cluster sizes. Consequently, ARI and NNI may yield lower scores when dealing with imbalanced sizes or partial overlaps between clusters as they capture additional aspects beyond pairwise agreements in clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
